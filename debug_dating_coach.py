import os
import streamlit as st
from volcenginesdkarkruntime import Ark
from zhipuai import ZhipuAI

# Default system prompt
DEFAULT_SYSTEM_PROMPT = """
# Role
ä½ æ˜¯ä¸€ä¸ªç»éªŒä¸°å¯Œçš„çº¦ä¼šæ•™ç»ƒï¼Œæ“…é•¿å¤„ç†å„ç§ç”·å¥³æƒ…æ„Ÿé—®é¢˜ï¼Œå¸®åŠ©å®¢æˆ·è§£å†³æ‹çˆ±å…³ç³»ä¸­çš„å°´å°¬å’Œç–‘æƒ‘ï¼Œå¹¶å¯ä»¥ä¸ºå®¢æˆ·æä¾›ä¸“ä¸šçš„çº¦ä¼šå»ºè®®ã€‚

# Task
ä½ çš„ä»»åŠ¡æ˜¯å›žç­”å®¢æˆ·å…³äºŽæƒ…æ„Ÿä»¥åŠçº¦ä¼šä¸­ç¢°åˆ°çš„é—®é¢˜ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºŽ
* çº¦ä¼šå»ºè®®
* å¦‚ä½•å¤„ç†å°´å°¬çš„åœºé¢
* å¦‚ä½•æå‡è‡ªå·±çš„é­…åŠ›
* å¦‚ä½•å¤„ç†æ‹çˆ±ä¸­çš„çŸ›ç›¾
* å¦‚ä½•æŒ½å›žæ„Ÿæƒ…
* å¦‚ä½•å›žå¤å¦ä¸€åŠæˆ–è€…è¿½æ±‚è€…çš„ä¿¡æ¯

# Constraints
* å¦‚æžœå®¢æˆ·æå‡ºçš„é—®é¢˜ä¸æ˜¯æƒ…æ„Ÿé—®é¢˜æˆ–è€…ä¸Žçº¦ä¼šæ— å…³ï¼Œä½ åº”å½“ç¤¼è²Œåœ°è¡¨ç¤ºæ— æ³•å›žç­”è¿™ç±»é—®é¢˜ã€‚

# Output
è¯·ç›´æŽ¥è¾“å‡ºä½ çš„å›žç­”ï¼Œä¸éœ€è¦æ·»åŠ é¢å¤–çš„æ ‡ç­¾æˆ–è€…æ ¼å¼
""".strip()

# App title
st.set_page_config(page_title="è°ƒè¯•çº¦ä¼šæ•™ç»ƒ", page_icon="ðŸ§Š")


def get_api_key(key_name: str) -> str:
    """Get API key"""
    if key_name in os.environ:
        api_key = os.environ.get(key_name)
    else:
        api_key = st.secrets[key_name]
    return api_key


def get_doubao_model_endpoint():
    """Get model endpoint for doubao LLM"""
    if "ARK_MODEL_ENDPOINT" in os.environ:
        model_endpoint = os.environ.get("ARK_MODEL_ENDPOINT")
    else:
        model_endpoint = st.secrets["ARK_MODEL_ENDPOINT"]
    return model_endpoint


with st.sidebar:
    st.title("è®¾ç½®")

    st.subheader("é€‰æ‹©æ¨¡åž‹ä¸Žå‚æ•°")
    selected_model = st.sidebar.selectbox(
        "Select a model",
        ["glm-4-plus", "glm-4-flash", "doubao-pro-4k"],
        key="selected_model",
    )
    if selected_model == "glm-4-plus" or selected_model == "glm-4-flash":
        llm = ZhipuAI(api_key=get_api_key("ZHIPUAI_API_KEY"))
        model = selected_model
    else:
        llm = Ark(api_key=get_api_key("ARK_API_KEY"))
        model = get_doubao_model_endpoint()

    temperature = st.sidebar.slider(
        "temperature", min_value=0.01, max_value=5.0, value=0.3, step=0.01
    )
    top_p = st.sidebar.slider(
        "top_p", min_value=0.01, max_value=1.0, value=0.9, step=0.01
    )

    st.subheader("Override system prompt")
    new_system_prompt = st.text_area("Override system prompt here...")


# Store LLM generated responses
if "messages" not in st.session_state.keys():
    st.session_state.messages = []

# Display system prompt
with st.expander(
    "System prompt (clear the overriding system prompt to fall back to the default)"
):
    current_system_prompt = DEFAULT_SYSTEM_PROMPT
    if new_system_prompt is not None and len(new_system_prompt.strip()) > 0:
        current_system_prompt = new_system_prompt
    st.text(current_system_prompt)


# Display or clear chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])


def clear_chat_history():
    st.session_state.messages = []


st.sidebar.button("æ¸…ç©ºèŠå¤©åŽ†å²", on_click=clear_chat_history)


def generate_response():
    """Call LLM to generate response"""
    system_prompt = DEFAULT_SYSTEM_PROMPT
    if new_system_prompt is not None and len(new_system_prompt.strip()) > 0:
        system_prompt = new_system_prompt.strip()
    messages = [{"role": "system", "content": system_prompt}]
    # prepare history
    for dict_message in st.session_state.messages:
        messages.append(dict_message)
    resp = llm.chat.completions.create(
        model=model,
        temperature=temperature,
        top_p=top_p,
        max_tokens=512,
        messages=messages,
        stream=False,
    )
    return resp.choices[0].message.content


# User-provided prompt
if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

# Generate a new response if last message is not from assistant
if (
    len(st.session_state.messages) > 0
    and st.session_state.messages[-1]["role"] == "user"
):
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            response = generate_response()
            placeholder = st.empty()
            full_response = ""
            for item in response:
                full_response += item
                placeholder.markdown(full_response)
            placeholder.markdown(full_response)
    message = {"role": "assistant", "content": full_response}
    st.session_state.messages.append(message)
